{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWwq/JwBlU9E55BlrTNbbZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varshitha-55/Sentiment-Analysis-using-NLP/blob/main/SENTIMENT_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDNXKsw1XbIm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout, Embedding, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('sentimentdataset.csv')\n",
        "\n",
        "# Printing shape of the dataset\n",
        "print(data.shape)\n",
        "# printing columns and rows information\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvvS--rpaN_w",
        "outputId": "1ede644a-8a79-4ad9-99bc-bbd355fdb189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(732, 2)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 732 entries, 0 to 731\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Text       732 non-null    object\n",
            " 1   Sentiment  732 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 11.6+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looking for NULL values\n",
        "print(\"Null Values:\\n\", data.isna().sum())\n",
        "\n",
        "# dropping null values\n",
        "data = data.dropna()\n",
        "\n",
        "# again checking for NULL values\n",
        "print(\"Null Values after dropping:\\n\", data.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGSrRTVhaOvg",
        "outputId": "37472088-3f13-4641-f40e-ada29f818e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null Values:\n",
            " Text         0\n",
            "Sentiment    0\n",
            "dtype: int64\n",
            "Null Values after dropping:\n",
            " Text         0\n",
            "Sentiment    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count of unique values in Sentiment column\n",
        "data['Sentiment'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "BxXvNv-_aT1V",
        "outputId": "dd3cc1cd-8075-420d-e40b-120b2d098e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment\n",
              "Positive               44\n",
              "Joy                    42\n",
              "Excitement             32\n",
              "Happy                  14\n",
              "Neutral                14\n",
              "                       ..\n",
              "Vibrancy                1\n",
              "Culinary Adventure      1\n",
              "Mesmerizing             1\n",
              "Thrilling Journey       1\n",
              "Winter Magic            1\n",
              "Name: count, Length: 279, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Positive</th>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Joy</th>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Excitement</th>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Happy</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Neutral</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Vibrancy</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Culinary Adventure</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mesmerizing</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Thrilling Journey</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Winter Magic</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>279 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the original category distribution\n",
        "print(\"Original Category Distribution:\")\n",
        "print(data['Sentiment'].value_counts())\n",
        "\n",
        "# Get the largest category size (i.e., the category with the maximum number of entries)\n",
        "max_size = data['Sentiment'].value_counts().max()\n",
        "\n",
        "# Perform oversampling\n",
        "balanced_df = data.groupby('Sentiment').apply(lambda x: x.sample(max_size, replace=True)).reset_index(drop=True)\n",
        "\n",
        "# Shuffle the dataset to avoid any order bias\n",
        "data = balanced_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Check the balanced category distribution\n",
        "print(\"\\nBalanced Category Distribution (After Oversampling):\")\n",
        "print(data['Sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezem3e6ujvnX",
        "outputId": "916c1a37-0527-4e3f-98af-eb64fc6e0e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Category Distribution:\n",
            "Sentiment\n",
            "Positive               44\n",
            "Joy                    42\n",
            "Excitement             32\n",
            "Happy                  14\n",
            "Neutral                14\n",
            "                       ..\n",
            "Vibrancy                1\n",
            "Culinary Adventure      1\n",
            "Mesmerizing             1\n",
            "Thrilling Journey       1\n",
            "Winter Magic            1\n",
            "Name: count, Length: 279, dtype: int64\n",
            "\n",
            "Balanced Category Distribution (After Oversampling):\n",
            "Sentiment\n",
            "Serenity            44\n",
            "Happy               44\n",
            "Blessed             44\n",
            "Intimidation        44\n",
            "Positivity          44\n",
            "                    ..\n",
            "Motivation          44\n",
            "Indifference        44\n",
            "Mischievous         44\n",
            "Joy in Baking       44\n",
            "Disgust             44\n",
            "Name: count, Length: 279, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading stopwords from nltk library\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# downloading punkt sentence tokenizer models\n",
        "nltk.download('punkt')\n",
        "# Downloading the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab') # This line is added to download the required data\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Review text Cleaning\n",
        "def clean_reviews(text):\n",
        "    if isinstance(text, str):  # Check if input is a string\n",
        "        # removing html brackets and other square brackets from the string using regex\n",
        "        regex = re.compile('<.*?>')  # r'<.*?>'\n",
        "        text = re.sub(regex, '', text)\n",
        "\n",
        "        # removing special characters like @, #, $, etc\n",
        "        pattern = re.compile('[^a-zA-z0-9\\s]')\n",
        "        text = re.sub(pattern, '', text)\n",
        "\n",
        "        # removing numbers\n",
        "        pattern = re.compile('\\d+')\n",
        "        text = re.sub(pattern, '', text)\n",
        "\n",
        "        # converting text to lower case\n",
        "        text = text.lower()\n",
        "\n",
        "        # Tokenization of words\n",
        "        text = word_tokenize(text)\n",
        "\n",
        "        # Stop words removal\n",
        "        text = [word for word in text if not word in stop_words]\n",
        "    return text\n",
        "\n",
        "# using the clean_reviews function on the dataset\n",
        "data['Text'] = data['Text'].apply(clean_reviews) # This is done in cell 13, avoid redundancy in cell 17"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBxn1pH_bjMf",
        "outputId": "72018e5a-3dd9-462e-d235-8afb823dc789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Text'][10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysQMdz4_htm9",
        "outputId": "82f4eb9c-9e96-4171-a5a2-046e9cee41fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drifting',\n",
              " 'day',\n",
              " 'air',\n",
              " 'nonchalance',\n",
              " 'indifferent',\n",
              " 'trivialities',\n",
              " 'life']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Sentiment'][10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zoQ37nT0iR2n",
        "outputId": "11dccf2e-d891-4f41-d29f-7fb60528344a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Indifference    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()"
      ],
      "metadata": {
        "id": "G18aj-40gaFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le.fit(data['Sentiment'])\n",
        "data['Sentiment'] = le.transform(data['Sentiment'])"
      ],
      "metadata": {
        "id": "GAGmgQmfgbdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'Text' column contains the features and 'Sentiment' column contains the target\n",
        "X = data['Text']\n",
        "y = data['Sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "hnwJ222eew8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.Sentiment.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfMdMa5dg2fb",
        "outputId": "688e47f2-b1f3-4f50-ce43-a4e79a40117e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([139,  34, 164, 215, 206,  19, 239,  82, 174, 192, 158, 193, 127,\n",
              "        11,  31, 120, 219,   6, 184,  93, 277, 218, 126, 247,  39, 171,\n",
              "       121,  30, 199,  76,  25, 187, 112, 195, 267,  89, 142, 249, 255,\n",
              "       101,  20,  17,  97,  44,  38,  54, 189,  70, 118, 229, 145, 210,\n",
              "       107,  90, 172, 104, 221, 140, 159, 211, 208, 100,  43,  74,  40,\n",
              "       259, 264,   8, 170,  78, 136,  71, 163,  96,  42, 115, 256,  47,\n",
              "       162, 231, 233,  23, 202, 180,  15,  56,  45, 225, 240, 124,  35,\n",
              "        14, 212, 133,  75, 169, 168, 144, 182,  91, 238, 250, 220,  28,\n",
              "        80,  53, 258, 128, 213,  52, 207, 103,  68,   4, 268, 131,  57,\n",
              "        72,   2,  21, 246, 223, 132,  22, 106, 248,  63, 181, 111, 117,\n",
              "        77, 244,  83,  46, 166, 176,  13, 102,  69, 214,  32, 167, 194,\n",
              "        29,  59, 198, 262, 116, 271, 205, 216,  36,  67, 186, 130,  33,\n",
              "       179, 270,  87,   7, 209, 137,  95,  41,  88,  86, 245, 153, 252,\n",
              "       217, 129,  49, 155, 156, 253, 109, 278, 261, 269,   0, 251, 134,\n",
              "       185, 177, 119,  27,  55, 147, 242, 228, 235, 234, 161,  62, 146,\n",
              "       203, 263, 224, 265,  24, 143,  16,  61, 123, 148, 196,   5, 204,\n",
              "       149, 150, 110,   1,  10, 232, 254,  98, 141,  94,  60, 274, 260,\n",
              "        99, 201,  84, 125, 135,  37, 188, 241, 191,  92,  58,  26, 197,\n",
              "       237,  50, 183, 266,   9, 160, 175, 151,  65,  51, 105, 222,  64,\n",
              "       275,  81,  18, 190,  66, 272,  48, 230, 273,  85, 257,  73, 165,\n",
              "       226, 152,   3, 138,  79, 276, 227,  12, 200, 108, 236, 154, 173,\n",
              "       178, 122, 113, 114, 157, 243])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Joining the list of tokens back into a string before fitting the TfidfVectorizer\n",
        "data['Text'] = data['Text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "tfidf.fit(data['Text'])\n",
        "requredTaxt  = tfidf.transform(data['Text'])"
      ],
      "metadata": {
        "id": "J-NoweBmg9bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0--77-7-jBDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(requredTaxt, data['Sentiment'], test_size=0.1, random_state=42)\n"
      ],
      "metadata": {
        "id": "7AHUkJQ4jHAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxltp9a0fSwv",
        "outputId": "a558fb13-60b0-4a91-fc6a-727aadd17ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11048, 2306)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0oa2vwvftZh",
        "outputId": "5b799c19-d3cb-413f-f7df-ccb484e16eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1228, 2306)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Ensure that X_train and X_test are dense if they are sparse\n",
        "X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
        "X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
        "\n",
        "# 1. Train KNeighborsClassifier\n",
        "knn_model = OneVsRestClassifier(KNeighborsClassifier())\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "print(\"\\nKNeighborsClassifier Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_knn)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_knn)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdZk1KurjXtx",
        "outputId": "6309beb4-76e1-4e3b-be59-df976c3383ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KNeighborsClassifier Results:\n",
            "Accuracy: 0.9837\n",
            "Confusion Matrix:\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 4 0 ... 0 0 0]\n",
            " [0 0 6 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 6 0 0]\n",
            " [0 0 0 ... 0 8 0]\n",
            " [0 0 0 ... 0 0 5]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         4\n",
            "           2       1.00      1.00      1.00         6\n",
            "           3       1.00      1.00      1.00         2\n",
            "           4       1.00      1.00      1.00         5\n",
            "           5       1.00      1.00      1.00         2\n",
            "           6       1.00      1.00      1.00         3\n",
            "           7       1.00      1.00      1.00         4\n",
            "           8       0.86      1.00      0.92         6\n",
            "           9       1.00      1.00      1.00         7\n",
            "          10       1.00      1.00      1.00         6\n",
            "          11       1.00      1.00      1.00         5\n",
            "          12       1.00      1.00      1.00         6\n",
            "          13       1.00      1.00      1.00         1\n",
            "          15       1.00      1.00      1.00         6\n",
            "          16       1.00      1.00      1.00         5\n",
            "          17       1.00      1.00      1.00         5\n",
            "          18       1.00      1.00      1.00         4\n",
            "          19       1.00      1.00      1.00         6\n",
            "          20       1.00      1.00      1.00         4\n",
            "          21       1.00      1.00      1.00         5\n",
            "          22       1.00      1.00      1.00         8\n",
            "          23       1.00      1.00      1.00        11\n",
            "          24       1.00      1.00      1.00         2\n",
            "          25       1.00      1.00      1.00         7\n",
            "          26       1.00      1.00      1.00         1\n",
            "          27       1.00      1.00      1.00         6\n",
            "          28       1.00      1.00      1.00         5\n",
            "          29       1.00      1.00      1.00         9\n",
            "          30       1.00      1.00      1.00         3\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       1.00      1.00      1.00         2\n",
            "          33       1.00      1.00      1.00         8\n",
            "          34       1.00      1.00      1.00         9\n",
            "          35       1.00      1.00      1.00         8\n",
            "          36       1.00      1.00      1.00         4\n",
            "          37       1.00      1.00      1.00         8\n",
            "          38       1.00      1.00      1.00         1\n",
            "          39       1.00      1.00      1.00         2\n",
            "          40       1.00      1.00      1.00         5\n",
            "          41       1.00      1.00      1.00         6\n",
            "          42       1.00      1.00      1.00         3\n",
            "          43       1.00      1.00      1.00         4\n",
            "          44       1.00      1.00      1.00         5\n",
            "          45       1.00      1.00      1.00         7\n",
            "          46       1.00      1.00      1.00         4\n",
            "          47       1.00      1.00      1.00         3\n",
            "          48       1.00      1.00      1.00         6\n",
            "          49       1.00      1.00      1.00         4\n",
            "          50       1.00      1.00      1.00         2\n",
            "          51       1.00      1.00      1.00         9\n",
            "          52       1.00      1.00      1.00         3\n",
            "          53       1.00      1.00      1.00         8\n",
            "          54       1.00      1.00      1.00         4\n",
            "          55       1.00      1.00      1.00         7\n",
            "          56       1.00      0.86      0.92         7\n",
            "          57       1.00      1.00      1.00         3\n",
            "          58       1.00      1.00      1.00         2\n",
            "          59       1.00      1.00      1.00         2\n",
            "          60       1.00      1.00      1.00         6\n",
            "          61       1.00      1.00      1.00         2\n",
            "          62       1.00      1.00      1.00         1\n",
            "          63       1.00      1.00      1.00         6\n",
            "          64       1.00      0.80      0.89         5\n",
            "          65       1.00      1.00      1.00         5\n",
            "          66       1.00      1.00      1.00         2\n",
            "          67       1.00      1.00      1.00         3\n",
            "          68       1.00      1.00      1.00         8\n",
            "          69       1.00      1.00      1.00         5\n",
            "          70       1.00      1.00      1.00         9\n",
            "          71       1.00      1.00      1.00         3\n",
            "          72       1.00      1.00      1.00         3\n",
            "          73       1.00      1.00      1.00         4\n",
            "          74       1.00      1.00      1.00         8\n",
            "          75       1.00      1.00      1.00         2\n",
            "          76       1.00      1.00      1.00         4\n",
            "          77       1.00      1.00      1.00         2\n",
            "          78       1.00      1.00      1.00         5\n",
            "          79       1.00      1.00      1.00         4\n",
            "          80       1.00      1.00      1.00         4\n",
            "          81       1.00      1.00      1.00         5\n",
            "          82       1.00      1.00      1.00         6\n",
            "          83       1.00      1.00      1.00         3\n",
            "          84       1.00      1.00      1.00         3\n",
            "          85       1.00      1.00      1.00         6\n",
            "          86       1.00      1.00      1.00         4\n",
            "          87       1.00      1.00      1.00         5\n",
            "          88       1.00      1.00      1.00         3\n",
            "          89       0.80      1.00      0.89         4\n",
            "          90       0.75      1.00      0.86         3\n",
            "          91       1.00      1.00      1.00         9\n",
            "          92       1.00      1.00      1.00         2\n",
            "          93       1.00      1.00      1.00         3\n",
            "          94       1.00      1.00      1.00         4\n",
            "          95       0.86      1.00      0.92         6\n",
            "          96       1.00      1.00      1.00         5\n",
            "          97       1.00      1.00      1.00         2\n",
            "          98       1.00      1.00      1.00         3\n",
            "          99       1.00      1.00      1.00         5\n",
            "         100       1.00      1.00      1.00         5\n",
            "         101       1.00      1.00      1.00         6\n",
            "         102       1.00      1.00      1.00         5\n",
            "         103       1.00      1.00      1.00         4\n",
            "         104       0.80      1.00      0.89         4\n",
            "         105       1.00      1.00      1.00         5\n",
            "         106       1.00      1.00      1.00         5\n",
            "         107       1.00      1.00      1.00         4\n",
            "         108       1.00      1.00      1.00         6\n",
            "         109       1.00      1.00      1.00         2\n",
            "         110       1.00      0.40      0.57         5\n",
            "         111       0.50      1.00      0.67         2\n",
            "         112       1.00      1.00      1.00         2\n",
            "         113       1.00      1.00      1.00         6\n",
            "         114       1.00      1.00      1.00         4\n",
            "         115       1.00      1.00      1.00         7\n",
            "         116       1.00      1.00      1.00         5\n",
            "         117       1.00      1.00      1.00         4\n",
            "         118       1.00      1.00      1.00         4\n",
            "         119       1.00      1.00      1.00         2\n",
            "         120       1.00      1.00      1.00         6\n",
            "         121       1.00      1.00      1.00         4\n",
            "         122       1.00      1.00      1.00         4\n",
            "         123       1.00      1.00      1.00         3\n",
            "         124       1.00      1.00      1.00         5\n",
            "         125       1.00      1.00      1.00         5\n",
            "         126       1.00      1.00      1.00         2\n",
            "         127       1.00      1.00      1.00         3\n",
            "         128       1.00      1.00      1.00         2\n",
            "         129       1.00      1.00      1.00         8\n",
            "         130       1.00      1.00      1.00         4\n",
            "         131       1.00      1.00      1.00         2\n",
            "         132       1.00      1.00      1.00         6\n",
            "         133       1.00      1.00      1.00         2\n",
            "         134       1.00      1.00      1.00         3\n",
            "         135       0.80      1.00      0.89         4\n",
            "         136       1.00      1.00      1.00         3\n",
            "         137       0.89      1.00      0.94         8\n",
            "         138       1.00      1.00      1.00         4\n",
            "         139       1.00      0.67      0.80         6\n",
            "         140       1.00      1.00      1.00         3\n",
            "         141       1.00      1.00      1.00         8\n",
            "         142       1.00      1.00      1.00         7\n",
            "         143       1.00      1.00      1.00         4\n",
            "         144       1.00      1.00      1.00         4\n",
            "         145       1.00      1.00      1.00         3\n",
            "         146       1.00      1.00      1.00         5\n",
            "         147       1.00      1.00      1.00         3\n",
            "         148       1.00      1.00      1.00         2\n",
            "         149       1.00      1.00      1.00         1\n",
            "         150       1.00      1.00      1.00         4\n",
            "         151       1.00      1.00      1.00         6\n",
            "         152       1.00      1.00      1.00         5\n",
            "         153       1.00      1.00      1.00         4\n",
            "         154       1.00      1.00      1.00         3\n",
            "         155       1.00      1.00      1.00         4\n",
            "         156       1.00      1.00      1.00         3\n",
            "         157       1.00      1.00      1.00         6\n",
            "         158       1.00      1.00      1.00         3\n",
            "         159       1.00      1.00      1.00         6\n",
            "         160       1.00      0.83      0.91         6\n",
            "         161       0.50      1.00      0.67         1\n",
            "         162       1.00      1.00      1.00         4\n",
            "         163       1.00      1.00      1.00         4\n",
            "         164       1.00      1.00      1.00         3\n",
            "         165       1.00      1.00      1.00         9\n",
            "         166       1.00      1.00      1.00         3\n",
            "         167       1.00      1.00      1.00         8\n",
            "         168       1.00      1.00      1.00         3\n",
            "         169       1.00      1.00      1.00         1\n",
            "         170       1.00      1.00      1.00         3\n",
            "         171       1.00      1.00      1.00         6\n",
            "         172       0.50      0.20      0.29         5\n",
            "         173       0.75      1.00      0.86         3\n",
            "         174       1.00      1.00      1.00         3\n",
            "         175       1.00      1.00      1.00         4\n",
            "         176       1.00      1.00      1.00         9\n",
            "         177       1.00      1.00      1.00         3\n",
            "         178       1.00      1.00      1.00         1\n",
            "         179       1.00      1.00      1.00         6\n",
            "         180       1.00      1.00      1.00         9\n",
            "         181       1.00      1.00      1.00         2\n",
            "         182       1.00      1.00      1.00         4\n",
            "         183       1.00      1.00      1.00         5\n",
            "         184       0.86      1.00      0.92         6\n",
            "         185       1.00      1.00      1.00         3\n",
            "         186       1.00      1.00      1.00         4\n",
            "         187       1.00      1.00      1.00         5\n",
            "         188       1.00      1.00      1.00         3\n",
            "         189       1.00      1.00      1.00         8\n",
            "         190       1.00      1.00      1.00         6\n",
            "         191       1.00      1.00      1.00         6\n",
            "         192       1.00      1.00      1.00         7\n",
            "         193       1.00      1.00      1.00         4\n",
            "         194       0.83      1.00      0.91         5\n",
            "         195       1.00      1.00      1.00         6\n",
            "         196       1.00      0.33      0.50         3\n",
            "         197       0.75      1.00      0.86         3\n",
            "         198       1.00      1.00      1.00         6\n",
            "         199       1.00      1.00      1.00         3\n",
            "         200       1.00      1.00      1.00         5\n",
            "         201       1.00      1.00      1.00         3\n",
            "         202       1.00      1.00      1.00         6\n",
            "         203       1.00      1.00      1.00         1\n",
            "         204       1.00      1.00      1.00         6\n",
            "         205       1.00      1.00      1.00         4\n",
            "         206       1.00      1.00      1.00         3\n",
            "         207       1.00      1.00      1.00         2\n",
            "         208       1.00      1.00      1.00         3\n",
            "         209       1.00      1.00      1.00         4\n",
            "         210       1.00      1.00      1.00         6\n",
            "         211       1.00      1.00      1.00         7\n",
            "         212       1.00      1.00      1.00         6\n",
            "         213       1.00      1.00      1.00         3\n",
            "         214       0.33      0.17      0.22         6\n",
            "         215       1.00      1.00      1.00         6\n",
            "         216       1.00      1.00      1.00         2\n",
            "         218       1.00      1.00      1.00         5\n",
            "         219       1.00      1.00      1.00         7\n",
            "         220       1.00      1.00      1.00         3\n",
            "         221       1.00      1.00      1.00         4\n",
            "         222       1.00      1.00      1.00         4\n",
            "         223       1.00      1.00      1.00         2\n",
            "         224       1.00      1.00      1.00         2\n",
            "         225       1.00      1.00      1.00         3\n",
            "         226       1.00      1.00      1.00         7\n",
            "         227       1.00      1.00      1.00         4\n",
            "         228       1.00      1.00      1.00         3\n",
            "         229       1.00      1.00      1.00         4\n",
            "         230       1.00      1.00      1.00         2\n",
            "         231       1.00      1.00      1.00         3\n",
            "         232       1.00      1.00      1.00         4\n",
            "         233       1.00      1.00      1.00         5\n",
            "         234       1.00      1.00      1.00         8\n",
            "         235       1.00      1.00      1.00         5\n",
            "         236       1.00      1.00      1.00         6\n",
            "         237       1.00      1.00      1.00         6\n",
            "         238       1.00      1.00      1.00         3\n",
            "         239       1.00      1.00      1.00         4\n",
            "         240       1.00      1.00      1.00         2\n",
            "         241       1.00      1.00      1.00         6\n",
            "         242       1.00      1.00      1.00         4\n",
            "         243       1.00      1.00      1.00         3\n",
            "         244       1.00      1.00      1.00         3\n",
            "         245       1.00      0.80      0.89         5\n",
            "         246       0.91      1.00      0.95        10\n",
            "         247       1.00      1.00      1.00         5\n",
            "         248       1.00      1.00      1.00         3\n",
            "         249       1.00      1.00      1.00         6\n",
            "         250       1.00      1.00      1.00         2\n",
            "         251       1.00      1.00      1.00         5\n",
            "         252       1.00      1.00      1.00         5\n",
            "         253       1.00      1.00      1.00         6\n",
            "         254       1.00      1.00      1.00         1\n",
            "         255       1.00      1.00      1.00         1\n",
            "         256       1.00      1.00      1.00         4\n",
            "         257       0.75      1.00      0.86         3\n",
            "         258       1.00      1.00      1.00         4\n",
            "         259       1.00      1.00      1.00         5\n",
            "         260       1.00      1.00      1.00         9\n",
            "         261       0.83      1.00      0.91         5\n",
            "         262       1.00      1.00      1.00         6\n",
            "         263       1.00      1.00      1.00         4\n",
            "         264       1.00      1.00      1.00         6\n",
            "         265       1.00      1.00      1.00         2\n",
            "         266       1.00      1.00      1.00         7\n",
            "         267       1.00      1.00      1.00         3\n",
            "         268       1.00      1.00      1.00         1\n",
            "         269       1.00      1.00      1.00         1\n",
            "         270       1.00      1.00      1.00         2\n",
            "         271       1.00      1.00      1.00         4\n",
            "         272       1.00      1.00      1.00         3\n",
            "         273       1.00      1.00      1.00         5\n",
            "         274       1.00      1.00      1.00         5\n",
            "         275       1.00      1.00      1.00         6\n",
            "         276       1.00      1.00      1.00         8\n",
            "         277       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.98      1228\n",
            "   macro avg       0.98      0.99      0.98      1228\n",
            "weighted avg       0.98      0.98      0.98      1228\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the category of a resume\n",
        "def pred(input_resume):\n",
        "    # Preprocess the input text (e.g., cleaning, etc.)\n",
        "    cleaned_text = clean_reviews(input_resume)\n",
        "\n",
        "    # Join the cleaned tokens back into a string\n",
        "    cleaned_text = ' '.join(cleaned_text) # Join the tokens into a string\n",
        "\n",
        "    # Vectorize the cleaned text using the same TF-IDF vectorizer used during training\n",
        "    vectorized_text = tfidf.transform([cleaned_text])\n",
        "\n",
        "    # Convert sparse matrix to dense\n",
        "    vectorized_text = vectorized_text.toarray()\n",
        "\n",
        "    # Prediction using rf_model, change to other model as needed\n",
        "    predicted_category = knn_model.predict(vectorized_text) # Changed to use rf_model\n",
        "\n",
        "    # get name of predicted category\n",
        "    predicted_category_name = le.inverse_transform(predicted_category)\n",
        "\n",
        "    return predicted_category_name[0]  # Return the category name"
      ],
      "metadata": {
        "id": "qE_wDDjvm94z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\" Reflecting on the past and looking ahead.\"\n",
        "pred(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tu7ZeXlVnYvd",
        "outputId": "df317296-d810-4937-e66a-00cc49235ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Intimidation    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\" The movie was very good.\"\n",
        "pred(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eDQluCG3n2tf",
        "outputId": "c2591d14-3c6a-4b6f-a549-189116156d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Excitement   '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\" Enjoying a beautiful day at the park!\"\n",
        "pred(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yDTzvXEBoFh1",
        "outputId": "833e2085-f98c-4947-89d0-6c473d7c3ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Love         '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Organizing a virtual talent show during challenging times, bringing smiles to classmates' faces! \"\n",
        "pred(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jMZc0B-8ohCC",
        "outputId": "6676f91d-8f9c-4be8-b152-2f29f0138ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Happy '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}